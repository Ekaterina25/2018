\documentclass[12pt]{article} 
\usepackage{ucs} 
\usepackage[utf8x]{inputenc}
\usepackage[russian]{babel} 
\usepackage{amssymb,amsmath}
\usepackage{hyperref} 
\numberwithin{equation}{section}
\textheight=24cm 
\textwidth=16cm 
\oddsidemargin=0pt 
\topmargin=-1.5cm
\date{}
\title{\bf Лекция 13\\
Коды, исправляющие ошибки\\
Параграф 13.2 Параметры и простейшие свойства кодов\\}
\author{Зимина Екатерина Андреевна}
\date{08/03/2018}


\begin{document} 
\maketitle
\newpage
\tableofcontents
\newpage
\section{ Параметры и простейшие свойства кодов}

Произвольное подмножество $G = \{{\bf {g}_1,\cdots,\bf {g}_m}\}$ множества $\bf{B}^n$ называется {\it двоичным кодом длины $n$}, а его элементы кодовыми словами или кодовыми векторами. Допустим, что при передаче по двоичному симметричному каналу кодовый вектор $g$ длины $n$ превратился в вектор $\upsilon$ такой же длины. В этом случае ненулевые компоненты вектора $\bf{c} = \bf{g}\bigoplus {\bf\upsilon}$ указывают положение ошибок, а сам вектор $\bf{c}$ называется вектором ошибок. Нетрудно видеть, что вероятность $p(\bf{c})$ появления вектора ошибок c в двоичном симметричном канале не зависит от передаваемого кодового слова, и для нее справедливо равенство 
\begin{equation}
p(\bf{c})~=~p^{||\bf{c}||}(1-p)^{n-||\bf c||},
\label{eq.simple} 
\end{equation}
где $||\bf{c}||$ — вес вектора $\bf{c}$. Так как ошибки происходят независимо и вероятность одной ошибки меньше $1/2$, то из уравнения~$\eqref{eq.simple}$, легко следует, что максимум условных вероятностей~$P(\bf{v}|\bf{g}_i)$ получить слово~$\bf v$ при условии, что было передано кодовое слово~$\bf{g}_i$, достигается на том слове~$\bf{g}$, расстояние Хемминга до которого от~$\bf v$ минимально. Если максимум достигается на единственном элементе кода~$G$, то говорят об исправлении ошибки в~$\bf v$. Определение для произвольного вектора~$\bf{v} \in B^n$ такого вектора~$\bf{g} \in G$, что расстояние от вектора~$\bf v$ до вектора~$\bf g$  меньше чем расстояние от~$\bf v$ до любого другого элемента G называется {\it декодированием} вектора~$\bf v$, или {\it исправлением ошибок} в векторе~$\bf v$. Если полученный в результате декодирования вектор~$\bf{g}$ совпадает с переданным вектором~$\bf{g}$, то декодирование называется правильным, если не совпадает — неправильным. Если же максимум условных вероятностей достигается более чем на одном кодовом слове, то говорят об обнаружении ошибки, исправить которую нельзя.


Каждому элементу~$\bf{g}$ кода G поставим в соответствие множество~$\bf{V}(\bf{g})$ всех тех наборов~$\bf{v}$ из $B^n$, для каждого из которых расстояние до~$\bf{g}$ строго меньше, чем расстояние до любого другого элемента кода G. Очевидно, что,если в результате ошибок передачи кодовое слово~$\bf{g}$ превращается в какой-~либо элемент~$\bf{v}$ из~$\bf{V}(\bf{g})$, то ошибки в слове~$\bf{v}$ можно исправить. Поэтому множество $C(\bf g) = \{ \bf c~|~\bf c = \bf g\bigoplus \bf v$~где~$\bf v \in V (\bf g)\}$ называется {\it множеством исправляемых ошибок кодового слова} $\bf{g}$ кода $G$. Нетрудно видеть, что сумма
\begin{equation}
\sum_{c\in C(\bf g)}p^{||\bf{c}||}(1-p)^{n-||\bf c||}
\label{eq.simple1} 
\end{equation}
равна вероятности исправления ошибки при передаче кодового слова $\bf g$. Минимум величин~$\eqref{eq.simple1}$, взятый по всем кодовым словам~$\bf g$, называется {\it вероятностью исправления ошибки} кодом $G$.

Как правило, множества $C(\bf g)$ имеют слишком сложную структуру, создающую значительные трудности при анализе кодов и их свойств. Большинство этих трудностей можно избежать, если, рассматривая коды, за-
менить множества~$C(\bf g)$ на их подмножества, являющиеся шарами одинакового для всех $\bf g$ радиуса. Такая замена приводит к понятиям кодового расстояния и кода с данным кодовым расстоянием.

Подмножество $\bf G = \{\bf g_1,...,g_m\}$ множества $\bf B_n$ называется {\it кодом длины $n$ с кодовым расстоянием $d$}, если для любых двух его элементов $g_i$ и $g_j$ расстояние Хемминга между ними не меньше $d$. Также говорят, что код~$G$ исправляет $t$ независимых ошибок, если его кодовое расстояние не меньше чем $2t + 1$. Нетрудно видеть, что в коде, исправляющем $t$ ошибок, каждый элемент кода является центром шара радиуса~$t$, и шары с центрами в разных элементах кода не пересекаются. Поэтомудля такого кода длины $n$ вероятность $P_e$ неправильного декодирования удовлетворяет неравенству
\begin{equation}
P_e \le \sum_{\bf c | ||\bf c ||>t}p^{||\bf{c}||}(1-p)^{n-||\bf c||} = \sum_{i=t+1}^n \begin{pmatrix} n \\ i  \end{pmatrix} p^i(1-p)^{n-i}.
\label{eq.simple2} 
\end{equation}
При заданной вероятности ошибки $p$ неравенство~$\eqref{eq.simple2}$ позволяет определить значения параметров $n$ и $t$, при которых вероятность неправильного декодирования $P_e$ принимает допустимые (с точки зрения решаемой при помощи кодирования задачи) значения. Например, используя неравенство Чебышева, можно показать, что для любого положительного числа $\delta$
\begin{equation}
P_e \rightarrow 0~\text{при}~ n \rightarrow \infty~\text{и}~ t = (1+\delta)pn
\label{eq.simple3} 
\end{equation}
Также можно показать$^{2)}$, что для любого положительного числа $\delta$
\begin{equation}
P_e \rightarrow 1~\text{при}~ n \rightarrow \infty~\text{и}~ t = (1-\delta)pn
\label{eq.simple4} 
\end{equation}
Для кода длины $n$ с кодовым расстоянием $d$ величину $d/n$ назовем {\it относительным кодовым расстоянием}. Из соотношений~$\eqref{eq.simple3}$
и~$\eqref{eq.simple4}$ можно сделать следующий качественный вывод о кодах, обеспечивающих надежную передачу информации по двоичному симметричному каналу: такие коды должны иметь большую длину, а их относительное кодовое расстояние должно быть константой.

Код $G$, исправляющий $t$ ошибок, называется {\it максимальным} для данного $t$, если мощность $G$ максимальна среди всех кодов, исправляющих $t$ ошибок. Покажем, что для мощности любого максимального кода~$G$ длины $n$ справедливы неравенства
\begin{equation}
\frac{2^n}{\sum_{i=0}^{2t} \begin{pmatrix} n \\ i  \end{pmatrix}} \rightarrow |G| \rightarrow \frac{2^n}{\sum_{i=0}^t \begin{pmatrix} n \\ i  \end{pmatrix}}.
\label{eq.simple5} 
\end{equation}

Так как шары радиуса $t$ с центрами в различных кодовых словах не пересекаются, то, очевидно, что для любого кода длины $n$ произведение
числа кодовых слов и мощности шара радиуса $t$ не превосходит $2^n$, и, следовательно, $|G|\le 2^{n}{\sum_{i=0}^t \begin{pmatrix} n \\ i  \end{pmatrix}}$. С другой стороны, для любого элемента $g$ кода $G$ в шаре радиуса $2t$ с центром в этом элементе есть ровно одно кодовое слово — элемент $\bf g$. Поэтому, если набор $\bf{a}$ не принадлежит ни одному из шаров радиуса $2t$ с центрами в элементах кода, то этот набор можно добавить в $G$, и дополненное множество по прежнемубудет исправлять $t$
ошибок. Поэтому, если $|G|({\sum_{i=0}^{2t} \begin{pmatrix} n \\ i  \end{pmatrix}}) < 2^n$, то код $G$ не является максимальным. Следовательно, неравенство $|G|({\sum_{i=0}^{2t} \begin{pmatrix} n \\ i  \end{pmatrix}})\ge 2^n$ является необходимым условием максимальности кода~$G$.
Для кода $G$ длины $n$ величина $R=\frac{log_2 |G|}{n}$ называется его {\itскоростью}.Выражая скорость максимального кода через его мощность из~$\eqref{eq.simple5}$, получим, что для скорости максимального кода длины~$n$ с исправлением $t$ ошибок справедливы неравенства

$1-\frac{log_2{\sum_{i=0}^{2t} \begin{pmatrix} n \\ i  \end{pmatrix}}}{n} \le R \le 1-\frac{log_2{\sum_{i=0}^{t} \begin{pmatrix} n \\ i  \end{pmatrix}}}{n}$,

которые при больших $n$ с помощью теорем 2.5 и 2.9 легко преобразуются к
виду
\begin{equation}
1-H\genfrac{(}{)}{}{}{2t}{n} \le R \le 1-H\genfrac{(}{)}{}{}{t}{n}+O\genfrac{(}{)}{}{}{log_2n}{n},
\label{eq.simple6} 
\end{equation}
где левое неравенство справедливо при $t\le n/4$, а правое при~$t\rightarrow \infty$~и~$t\le n/2$. Объединяя неравенства~$\eqref{eq.simple3}$~и~$\eqref{eq.simple6}$, заключаем, что при помощи кодов по двоичному симметричному каналу с вероятностью ошибки~$p$ можно передавать информацию с близкой к нулю вероятностью неправильного декодирования и скоростью
\begin{equation}
1-H((1+\delta)2p) \le R \le 1-H((1+\delta)p),
\label{eq.simple7} 
\end{equation}
где $\delta$ — сколь угодно малое положительное число, удовлетворяющее неравенству $(1 + \delta)2p \le 1/2$.

Доказательство нижней оценки в~$\eqref{eq.simple5}$ фактически содержит алгоритм построения кода $G$, на котором эта оценка достигается, но к сожалению ничего не говорит о каких-либо свойствах этого кода, знание которых могло бы помочь в построении хороших алгоритмов кодирования и декодирования. Поэтомукодирование $G$ будем рассматривать как произвольную булеву функцию $f : \{0, 1\}^{| log_2 |G||}\rightarrow \{0, 1\}^n$. При фиксированном $p$ и $n →\infty$ сложность такой функции есть
$O(n|G|/ log_2 n|G|)$. Для декодирования кода $G$ можно вычислить расстояния от принятого вектора до всех кодовых слов и выбрать слово с минимальным расстоянием. Сложность такого декодирования есть $O(n|G|)$. Существуют другие, более экономные с точки зрения числа операций алгоритмы декодирования произвольных кодов. Однако все эти алгоритмы используют тот или иной перебор кодовых слов и требуют для своей работы память, способную вместить $|G|$ двоичных наборов длины $n$. Так как малая вероятность безошибочной передачи информации достигается при использовании кодов большой длины, то указанные методы кодирования и декодирования не представляют практического интереса.

Часто декодирование кода, исправляющего $t$ ошибок, можно существенно упростить, если исправлять ошибки только в тех случаях, когда число
ошибок не превосходит $t$. Если же декодируемое слово находится на расстоянии большем чем $t$ от любого кодового слова, то в этом случае говорят об обнаружении ошибки, которую нельзя исправить. Такое декодирование называется декодированием в пределах кодового расстояния и именно такому декодированию соответствует приведенная выше оценка~$\eqref{eq.simple2}$ вероятности неправильного декодирования.

\end{document}